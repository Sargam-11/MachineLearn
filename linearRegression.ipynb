{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *importing the libraries*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection  import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('fuel.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  2.1 describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2.2 histogram : data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (3,3),color=['purple'])\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data = df,hue='CYLINDERS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df = df.select_dtypes(include='number')\n",
    "corrmatrix = n_df.corr()\n",
    "print(corrmatrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot = True,cmap = 'BuPu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 deleting duplicate records/rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 (a) feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.iloc[:,[4,5,8,9,10,11,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = (d - d.min())/(d.max()-d.min())\n",
    "normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 (b) z-score indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want we perform feature scaling as we donot want a set of features ( features taking larger scale of values ) to dominate the prediction / estimation as compared to  other set of features ( features taking lower scale of values ).\n",
    "# (x - xmean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = (d - d.mean())/d.std()\n",
    "zscore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "XScaled = sc.fit_transform(zscore)\n",
    "XScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = zscore.iloc[:,[0]]\n",
    "extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(extracted.iloc[:,0].values,df.loc[:,'CO2EMISSIONS'].values,test_size = 0.2,random_state = 101) \n",
    "x_train.size,x_test.size,y_train.size,y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = (np.mean(x_train * y_train) - (np.mean(x_train) * np.mean(y_train))) / (np.mean(x_train ** 2) - (np.mean(x_train))** 2)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = y_train.mean() - w * x_train.mean()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train,y_train,c='grey')\n",
    "y_pred = ( w * x_train + b )\n",
    "plt.plot(x_train,y_pred, c =  'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 (c) normal equation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = (X^TX)^-1 . X^TY\n",
    "Y = y_train.reshape(853,1)\n",
    "X = x_train.reshape(853,1)\n",
    "onesArray = np.ones(X.shape)\n",
    "Y.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((onesArray,X))\n",
    "inverse = np.linalg.inv(X.T.dot(X))\n",
    "\n",
    "dablu = inverse.dot(X.T.dot(Y))\n",
    "dablu\n",
    "# inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, Y, w, b):\n",
    "    #### Compute cost J\n",
    "    J = (((w*np.mean(X) + b) - np.mean(Y))**2)/(2*X.shape[0])\n",
    "    return J\n",
    "\n",
    "def gradientDescent (X, Y, learningRate, numIterations):\n",
    "    w, b = 0, 0\n",
    "    errorList =[] ### Store cost in each iteration\n",
    "    for i in range(numIterations):\n",
    "        print(f'iteration {i} value pf w = {w} and b = {b}')\n",
    "#         Y_pred = #COMPUTE f(X)\n",
    "        ### compute updated w and b\n",
    "        costJ = computeCost(X, Y, w, b)\n",
    "        errorList.append(costJ)\n",
    "        wc = w - learningRate*(w*(np.mean((X)**2)) + b*np.mean(X) - np.mean(X*Y))\n",
    "        bc = b - learningRate * (w* np.mean(X) + b - np.mean(Y))\n",
    "        w = wc\n",
    "        b = bc\n",
    "    return (w,b,errorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w,b,errorList = gradientDescent (x_train, y_train, 0.001, numIterations=5000)\n",
    "# Match w and b against the ones obtained in A and B part\n",
    "# Plot iteration vs error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempX = df.loc[:,['ENGINESIZE','FUELCONSUMPTION_COMB']]\n",
    "tempX.shape[0]\n",
    "tempY = df[['CO2EMISSIONS']]\n",
    "multxScaled = (tempX - tempX.mean())/ tempX.std()\n",
    "multxScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xMult_train,xMult_test,yMult_train,yMult_test =  train_test_split(multxScaled,tempY,test_size = 0.2,random_state = 101)\n",
    "xMult_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiX = np.hstack((np.ones((xMult_train.shape[0],1)),xMult_train))\n",
    "multiX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T1 = np.linalg.inv(multiX.T.dot(multiX))\n",
    "\n",
    "T2 = multiX.T.dot(yMult_train)\n",
    "multW = T1.dot(T2)\n",
    "multW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent for multivarient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "print(xMult_train)\n",
    "def gradientDescent (X, Y, learningRate, numIterations):\n",
    "    w1,w2 = 0,0\n",
    "    b = 0\n",
    "    errorList =[] \n",
    "    for i in range(numIterations):\n",
    "        print(\"X : \",X,Y)\n",
    "        Y_pred = w1 * X.iloc[:,0] + w2 * X.iloc[:,1] + b\n",
    "        print(Y.values.squeeze().shape)\n",
    "        ### compute updated w and b\n",
    "        wd1 = (2/n) * np.sum((Y_pred - Y)*X.iloc[:,0])\n",
    "        wd2 = (2/n) * np.sum((Y_pred - Y)*X.iloc[:,1])\n",
    "        bd = (2/n) * np.sum(Y_pred - Y)\n",
    "        costJ = (1/n) * np.sum([val**2 for val in (Y.values.squeeze() - Y_pred)])\n",
    "        w1 = w1 - learningRate * wd1\n",
    "        w2 = w2 - learningRate * wd2\n",
    "        b = b - learningRate * bd\n",
    "        errorList.append(costJ)\n",
    "    return (w1,w2,b,errorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2,b,errorList = gradientDescent (xMult_train, yMult_train, 0.001, numIterations=2)\n",
    "# Match w and b against the ones obtained in A and B part\n",
    "# Plot iteration vs error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, b = 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[val for val in (yMult_train - (w1 * xMult_train.iloc[:,0] + w2 * xMult_train.iloc[:,1] + b))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yMult_train. - (w1 * xMult_train.iloc[:,0] + w2 * xMult_train.iloc[:,1] + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yMult_train.values.squeeze() - (w1 * xMult_train.iloc[:,0] + w2 * xMult_train.iloc[:,1]) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
